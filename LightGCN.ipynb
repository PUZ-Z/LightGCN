{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GCN 논문 리뷰 & 코드 작성\n",
    "**LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation**  \n",
    "*He et al. (2020)*  \n",
    "🔗 [논문 링크](https://arxiv.org/abs/2002.02126)\n",
    "\n",
    "---\n",
    "\n",
    "## 핵심 수식\n",
    "\n",
    "유저와 아이템의 초기 임베딩을 각각 $e_u^{(0)}, e_i^{(0)}$ 라고 할 때, k번째 layer에서 다음과 같이 업데이트 된다.\n",
    "\n",
    "$$\n",
    "e_u^{(k+1)} = \\sum_{i\\in{N_n}}\\frac{1}{\\sqrt{|N_u||N_i|}}e_i^{(k)}\n",
    "$$\n",
    "\n",
    "마지막에는 여러 레이어의 임베딩을 평균 or 가중합 해서 최종 임베딩을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 설정 (Hyperparameter)\n",
    "\n",
    "|항목|설정|\n",
    "|:---:|:---:|\n",
    "|임베딩 차원|64|\n",
    "|학습률|0.001|\n",
    "|optimizer|Adam|\n",
    "|weight decay|1e-4(정규화 항으로 BPR에 포함)|\n",
    "|negative sampling|1:1 비율로 sampling|\n",
    "|배치  사이즈|1024(Amazon Books : 2048)|\n",
    "|학습 epoch|max 1000, 일반적으로 200~400|\n",
    "|레이어 수|3|\n",
    "|초기화|Xavier uniform|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'embedding_dim': 64,\n",
    "    'num_layers': 3,\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 200,\n",
    "    'patience': 10,\n",
    "    'eval_k': 20,\n",
    "    'reg_lambda': 1e-4,\n",
    "    'debug': False,\n",
    "    'dropout': False,\n",
    "    'keep_prob': 0.6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Users: 52643, # Interactions: 2380730, # Items: 91599\n"
     ]
    }
   ],
   "source": [
    "# train.txt 로드 → user_item_dict\n",
    "user_item_dict = defaultdict(set)\n",
    "with open(\"data/train.txt\") as f:\n",
    "    for user, line in enumerate(f):\n",
    "        items = list(map(int, line.strip().split()))\n",
    "        for item in items[1:]:\n",
    "            user_item_dict[user].add(item)\n",
    "\n",
    "# test.txt 로드 → test_ground_truth\n",
    "test_ground_truth = {}\n",
    "\n",
    "with open(\"data/test.txt\") as f:\n",
    "    for user, line in enumerate(f):\n",
    "        parts = list(map(int, line.strip().split()))\n",
    "        if len(parts) < 2:\n",
    "            continue  # 유저 ID만 있는 경우는 건너뛰기\n",
    "        items = parts[1:]  # 유저 ID 뒤에 나오는 모든 아이템\n",
    "        test_ground_truth[user] = items\n",
    "\n",
    "\n",
    "# train_interactions 생성 (for model input)\n",
    "train_user, train_item = [], []\n",
    "for user, items in user_item_dict.items():\n",
    "    for item in items:\n",
    "        train_user.append(user)\n",
    "        train_item.append(item)\n",
    "\n",
    "train_interactions = torch.stack([\n",
    "    torch.tensor(train_user),\n",
    "    torch.tensor(train_item)\n",
    "], dim=0)\n",
    "\n",
    "# 사용자-아이템 딕셔너리로부터 유저/아이템 수 계산\n",
    "all_items = set()\n",
    "for items in user_item_dict.values():\n",
    "    all_items.update(items)\n",
    "for items in test_ground_truth.values():\n",
    "    all_items.update(items)\n",
    "num_items = max(all_items) + 1\n",
    "num_users = len(user_item_dict)\n",
    "\n",
    "# 통계 확인\n",
    "print(f\"# Users: {num_users}, # Interactions: {len(train_user)}, # Items: {num_items}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGCN 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    # 모델이 사용할 유저/아이템 수, 임베딩 차원, GCN 레이어 수, 엣지 구조를 초기화하는 부분\n",
    "    def __init__(self, num_users, num_items, embedding_dim, num_layers, user_item_pairs):\n",
    "        super(LightGCN, self).__init__()\n",
    "        \n",
    "        # 하이퍼파라미터를 멤버 변수로 저장\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 유저, 아이템 각각에 대한 학습 가능한 임베딩 벡터 정의\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # 임베딩 가중치 초기화(논문에서 Xavier 초기화 사용)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "\n",
    "        # 엣지 구조 저장\n",
    "        self.norm_adj = self.build_adj_matrix(user_item_pairs)\n",
    "\n",
    "    # 유저-아이템 상호작용 데이터를 기반으로 정규화된 인접 행렬을 만들기 위함\n",
    "    def build_adj_matrix(self, user_item_pairs):\n",
    "        num_nodes = self.num_users + self.num_items # 노드 개수 설정\n",
    "        \n",
    "        # bipartite graph 만들기\n",
    "        rows = user_item_pairs[0].cpu().numpy()\n",
    "        cols = user_item_pairs[1].cpu().numpy() + self.num_users\n",
    "        \n",
    "        R = coo_matrix((np.ones(len(rows)), (rows, cols)), shape=(num_nodes, num_nodes)) # 엣지 가중치를 전부 1.0으로 설정되고 있음\n",
    "        \n",
    "        # 양방향으로 연결(symmetric adjacency)\n",
    "        adj = R + R.T\n",
    "\n",
    "        # 정규화\n",
    "        rowsum = np.array(adj.sum(1)).flatten() # degree 계산\n",
    "        d_inv_sqrt = np.power(rowsum + 1e-8, -0.5) # degree matrix D 계산 -> 역루트 취해서 정규화에 사용\n",
    "        d_mat_inv_sqrt = coo_matrix((d_inv_sqrt, (np.arange(num_nodes), np.arange(num_nodes))), shape=(num_nodes, num_nodes))\n",
    "\n",
    "        norm_adj = d_mat_inv_sqrt @ adj @ d_mat_inv_sqrt # 정규화된 sparse adjacency matrix\n",
    "        return self._convert_sp_mat_to_sp_tensor(norm_adj)\n",
    "\n",
    "    # scipy.sparse.coo_matrix -> torch.sparse.FloatTensor로 바꾸는 역할\n",
    "    def _convert_sp_mat_to_sp_tensor(self, mat):\n",
    "        mat = mat.tocoo().astype(np.float32) # coo 형식으로 변환하고 타입을 float32로 변환\n",
    "        indices = torch.from_numpy(np.vstack((mat.row, mat.col))).long() # (row, col) 인덱스를 Pytorch tensor로 변환\n",
    "        values = torch.from_numpy(mat.data) # 각 엣지의 값(가중치)을 tensor로 변환\n",
    "        shape = torch.Size(mat.shape) # 전체 sparse tensor의 shape 결정\n",
    "        return torch.sparse_coo_tensor(indices, values, shape).to(self.user_embedding.weight.device) # sparse tensor로 생성하고 모델이 사용하는 디바이스로 이동\n",
    "\n",
    "    # 유저/아이템 초기 임베딩을 그래프 전파를 통해 업데이트하고 평균 임베딩 변환\n",
    "    def getEmbedding(self):\n",
    "        all_emb = torch.cat([self.user_embedding.weight, self.item_embedding.weight]) # 유저/아이템 초기 임베딩 연결 (Layer0)\n",
    "        embs = [all_emb] # 각 레이어별 임베딩 리스트 초기화\n",
    "        adj = self.norm_adj\n",
    "        \n",
    "        # LightGCN message passing : 각 레이어마다 임베딩 업데이트\n",
    "        for _ in range(self.num_layers):\n",
    "            all_emb = torch.sparse.mm(adj, all_emb)\n",
    "            embs.append(all_emb)\n",
    "        \n",
    "        embs = torch.stack(embs, dim=1) # 모든 레이어의 임베딩을 쌓고,\n",
    "        light_out = torch.mean(embs, dim=1) # 레이어별 임베딩 평균\n",
    "        user_emb, item_emb = light_out[:self.num_users], light_out[self.num_users:] # 유저/아이템 임베딩 분리해서 반환\n",
    "        return user_emb, item_emb\n",
    "\n",
    "    # 모든 아이템에 대한 평점 예측 점수 반환 \n",
    "    def getUsersRating(self, users):\n",
    "        user_emb, item_emb = self.getEmbedding() # GCN 기반으로 유저/아이템 임베딩 추출\n",
    "        users_emb = user_emb[users] # 각 유저/positive/negative 아이템의 GCN 임베딩 추출\n",
    "        scores = torch.matmul(users_emb, item_emb.t()) # 유저 임베딩과 전체 아이템 임베딩 간 내적하여 평점 예측 점수 추출\n",
    "        return torch.sigmoid(scores) # Sigmoid를 통해 점수를 0~1 범위로 정규화\n",
    "\n",
    "    # 유저, positive item, negative item 임베딩을 각각 추출 + 초기 임베딩도 반환\n",
    "    def getEmbeddingTriple(self, users, pos_items, neg_items):\n",
    "        user_emb, item_emb = self.getEmbedding()\n",
    "        users_emb = user_emb[users]\n",
    "        pos_emb = item_emb[pos_items]\n",
    "        neg_emb = item_emb[neg_items]\n",
    "        \n",
    "        users_emb_0 = self.user_embedding(users)\n",
    "        pos_emb_0 = self.item_embedding(pos_items)\n",
    "        neg_emb_0 = self.item_embedding(neg_items)\n",
    "        \n",
    "        return users_emb, pos_emb, neg_emb, users_emb_0, pos_emb_0, neg_emb_0\n",
    "\n",
    "    # GCN 레이어를 반복하면서 유저/아이템의 임베딩을 업데이트하고 마지막에 평균을 내서 최종 임베딩을 출력하는 것\n",
    "    def forward(self):\n",
    "        # 초기 임베딩 가져오기 (Layer 0)\n",
    "        emb = torch.cat([self.user_embedding.weight, self.item_embedding.weight], dim=0)\n",
    "        all_embs = [emb]\n",
    "        \n",
    "        adj = self.norm_adj\n",
    "        \n",
    "        # 메시지 전달 (Layer 수 만큼 반복)\n",
    "        for _ in range(self.num_layers):\n",
    "            emb = torch.sparse.mm(adj, emb)\n",
    "            all_embs.append(emb)\n",
    "        \n",
    "        # 레이어별 임베딩 평균\n",
    "        final_emb = torch.stack(all_embs, dim=0).mean(0)\n",
    "        \n",
    "        # 유저/아이템 임베딩 분리\n",
    "        user_emb, item_emb = final_emb[:self.num_users], final_emb[self.num_users:]\n",
    "        return user_emb, item_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR Loss\n",
    "\n",
    "$$L_{BPR}=-\\sum_{(u,i,j)}log\\;\\sigma(\\hat y_{ui}-\\hat y_{uj})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(model, users, pos_items, neg_items):\n",
    "    # GCN 결과 + 초기 임베딩 모두 가져오기\n",
    "    users_emb, pos_emb, neg_emb, user_emb_0, pos_emb_0, neg_emb_0 = model.getEmbeddingTriple(users.long(), pos_items.long(), neg_items.long())\n",
    "\n",
    "    # BPR loss 계산\n",
    "    pos_scores = torch.sum(users_emb * pos_emb, dim=1)\n",
    "    neg_scores = torch.sum(users_emb * neg_emb, dim=1)\n",
    "    loss = torch.mean(F.softplus(neg_scores - pos_scores))\n",
    "\n",
    "    # 초기 임베딩에만 정규화 적용 (논문과 동일)\n",
    "    reg_loss = config['reg_lambda'] * (user_emb_0.norm(2).pow(2) + pos_emb_0.norm(2).pow(2) + neg_emb_0.norm(2).pow(2)) / users.shape[0]\n",
    "\n",
    "    return loss + reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=config['patience'], delta=0.0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None or current_score > self.best_score + self.delta:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 지표 & 함수\n",
    "\n",
    "해당 논문에서는 Recall@20, Precision@20, NDCG@20 으로 진행하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall@K : 정답 아이템 중 상위 K개에 포함된 비율\n",
    "def recall_at_k(ranked_list, ground_truth, k):\n",
    "    return len(set(ranked_list[:k]) & set(ground_truth)) / len(set(ground_truth))\n",
    "\n",
    "# Precision@K : 상위 K개 중 정답 아이템의 비율\n",
    "def precision_at_k(ranked_list, ground_truth, k):\n",
    "    return len(set(ranked_list[:k]) & set(ground_truth)) / k\n",
    "\n",
    "# NDCG@K : 정답 아이템의 순위를 고려한 정밀도 지표\n",
    "def ndcg_at_k(ranked_list, ground_truth, k):\n",
    "    dcg = 0.0\n",
    "    for i, item in enumerate(ranked_list[:k]):\n",
    "        if item in ground_truth:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    idcg = sum(1 / np.log2(i + 2) for i in range(min(len(ground_truth), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "# 전체 유저에 대해 모델 성능 평가 (배치 단위)\n",
    "def evaluate_model(model, test_ground_truth, user_item_dict, k=20, batch_size=1024, silent=False, desc=\"Evaluating\"):\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_emb, item_emb = model()\n",
    "        user_emb = user_emb.to(device)\n",
    "        item_emb = item_emb.to(device)\n",
    "\n",
    "    recall_list, precision_list, ndcg_list = [], [], []\n",
    "    test_users = list(test_ground_truth.keys())\n",
    "\n",
    "    for i in tqdm(range(0, len(test_users), batch_size), desc=desc):\n",
    "        batch_users = test_users[i:i+batch_size]\n",
    "        batch_user_emb = user_emb[batch_users]\n",
    "\n",
    "        for idx, user in enumerate(batch_users):\n",
    "            gt_items = test_ground_truth[user]\n",
    "            train_items = user_item_dict.get(user, set())\n",
    "            candidates = list((set(range(model.num_items)) - train_items) | set(gt_items))\n",
    "\n",
    "            scores = torch.matmul(batch_user_emb[idx], item_emb[candidates].T)\n",
    "            ranked_items = [candidates[i] for i in torch.topk(scores, k).indices.tolist()]\n",
    "\n",
    "            recall_list.append(recall_at_k(ranked_items, gt_items, k))\n",
    "            precision_list.append(precision_at_k(ranked_items, gt_items, k))\n",
    "            ndcg_list.append(ndcg_at_k(ranked_items, gt_items, k))\n",
    "\n",
    "    if not silent:\n",
    "        print(f\"Recall@{k}: {np.mean(recall_list):.4f}, Precision@{k}: {np.mean(precision_list):.4f}, NDCG@{k}: {np.mean(ndcg_list):.4f}\")\n",
    "\n",
    "    return np.mean(recall_list), np.mean(precision_list), np.mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 개의 배열을 동일한 순서로 무작위 셔플\n",
    "def shuffle(*arrays):\n",
    "    if len(set(len(x) for x in arrays)) != 1:\n",
    "        raise ValueError(\"All arrays must have the same length\")\n",
    "    idx = np.random.permutation(len(arrays[0]))\n",
    "    return tuple(x[idx] for x in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저별 (user, pos, neg) 트리플 샘플 생성\n",
    "def sample_user_triplets(user_item_dict, all_items, users):\n",
    "    samples = []\n",
    "    for u in users:\n",
    "        if not user_item_dict[u]:\n",
    "            continue\n",
    "        pos = random.choice(list(user_item_dict[u]))\n",
    "        neg_pool = list(all_items - user_item_dict[u])\n",
    "        if not neg_pool:\n",
    "            continue\n",
    "        neg = random.choice(neg_pool)\n",
    "        samples.append((u, pos, neg))\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 단위로 모델을 학습하고 평균 loss 반환\n",
    "def train_one_epoch(model, optimizer, user_item_dict, all_items, batch_size, device):\n",
    "    model.train()\n",
    "    users = list(user_item_dict.keys())\n",
    "    random.shuffle(users)\n",
    "    samples = sample_user_triplets(user_item_dict, all_items, users)\n",
    "\n",
    "    users_tensor = torch.tensor([s[0] for s in samples], device=device)\n",
    "    pos_tensor = torch.tensor([s[1] for s in samples], device=device)\n",
    "    neg_tensor = torch.tensor([s[2] for s in samples], device=device)\n",
    "\n",
    "    users_tensor, pos_tensor, neg_tensor = shuffle(users_tensor, pos_tensor, neg_tensor)\n",
    "\n",
    "    total_loss = 0\n",
    "    num_batches = len(users_tensor) // batch_size + 1\n",
    "\n",
    "    for start in range(0, len(users_tensor), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_users = users_tensor[start:end]\n",
    "        batch_pos = pos_tensor[start:end]\n",
    "        batch_neg = neg_tensor[start:end]\n",
    "\n",
    "        loss = bpr_loss(model, batch_users, batch_pos, batch_neg)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 학습 과정을 실행하고, TensorBoard 로그 및 EarlyStopping 포함\n",
    "def train_lightgcn(model, interactions, optimizer, test_ground_truth, user_item_dict,\n",
    "                   epochs=config['epochs'], batch_size=config['batch_size'], k=config['eval_k'], patience=config['patience']):\n",
    "    writer = SummaryWriter()\n",
    "    topks = [k]\n",
    "\n",
    "    # interaction을 기반으로 유저-아이템 딕셔너리 구성\n",
    "    user_item_dict.clear()\n",
    "    for u, i in zip(interactions[0], interactions[1]):\n",
    "        user_item_dict[u.item()].add(i.item())\n",
    "\n",
    "    all_items = set(range(model.num_items))\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"[Epoch {epoch+1}] Training start...\")\n",
    "        avg_loss = train_one_epoch(model, optimizer, user_item_dict, all_items, batch_size, device)\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {avg_loss:.4f}\")\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "\n",
    "        # 평가 및 로깅\n",
    "        recall_all = {}\n",
    "        for topk in topks:\n",
    "            recall, prec, ndcg = evaluate_model(model, test_ground_truth, user_item_dict, topk, silent=True)\n",
    "            writer.add_scalars(f\"Metrics@{topk}\", {\"Recall\": recall, \"Precision\": prec, \"NDCG\": ndcg}, epoch)\n",
    "            recall_all[topk] = recall\n",
    "\n",
    "        # EarlyStopping 체크\n",
    "        recall = recall_all[k]\n",
    "        early_stopping(recall)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 학습 및 평가 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Training start...\n",
      "Epoch 1: Loss = 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [09:12<00:00, 10.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Training start...\n",
      "Epoch 2: Loss = 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [25:08<00:00, 29.00s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Training start...\n",
      "Epoch 3: Loss = 0.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [32:12<00:00, 37.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Training start...\n",
      "Epoch 4: Loss = 0.5537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [09:18<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Training start...\n",
      "Epoch 5: Loss = 0.5061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 52/52 [09:36<00:00, 11.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Training start...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_lightgcn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_interactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ground_truth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_item_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval_k\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 평가 성능 (K=20)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m evaluate_model(model, test_ground_truth, user_item_dict, k\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_k\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m, in \u001b[0;36mtrain_lightgcn\u001b[0;34m(model, interactions, optimizer, test_ground_truth, user_item_dict, epochs, batch_size, k, patience)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Training start...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_item_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_loss, epoch)\n",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, user_item_dict, all_items, batch_size, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m bpr_loss(model, batch_users, batch_pos, batch_neg)\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "model = LightGCN(num_users, num_items, embedding_dim=config['embedding_dim'], num_layers=config['num_layers'], user_item_pairs=train_interactions)\n",
    "\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "# 모델 학습\n",
    "train_lightgcn(\n",
    "    model,\n",
    "    train_interactions,\n",
    "    optimizer,\n",
    "    test_ground_truth,\n",
    "    user_item_dict,\n",
    "    epochs=config['epochs'],\n",
    "    batch_size=config['batch_size'],\n",
    "    k=config['eval_k'],\n",
    "    patience=config['patience']\n",
    ")\n",
    "\n",
    "# 평가 성능 (K=20)\n",
    "evaluate_model(model, test_ground_truth, user_item_dict, k=config['eval_k'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall@20: 0.0687, Precision@20: 0.0034, NDCG@20: 0.0260"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
